主要参照《unix网络编程卷1 第三版》及《Linux高性能服务器编程》
在Linux下eclipse中进行开发，unpthread头文件主要包括了常用套接字函数的包裹函数，简化了程序的编写，具体详见《unix网络编程卷1 第三版》的引言，可从网上下载。
主要的想法是参照《Linux高性能服务器编程》中第八章中关于epoll模式的讲解，对《unix网络编程卷1 第三版》中第30章中线程池服务器程序进行改写，其原代码如下所示。

    
#include "unpthread.h"    

typedef struct {    
  pthread_t		thread_tid;		/* thread ID */
  long			thread_count;	/* # connections handled */
} Thread;
Thread	*tptr;		/* array of Thread structures; calloc'ed */
    
#define	MAXNCLI	32
int		   clifd[MAXNCLI], iget, iput,*iptr;

static int			nthreads;
pthread_mutex_t		clifd_mutex = PTHREAD_MUTEX_INITIALIZER;
pthread_cond_t		clifd_cond = PTHREAD_COND_INITIALIZER;

#define	MAXN	16384		/* max # bytes client can request */

void web_child(int sockfd)
{
	int			ntowrite;
	ssize_t		nread;
	char		line[MAXLINE], result[MAXN];

	for ( ; ; ) {
		if ( (nread = Readline(sockfd, line, MAXLINE)) == 0)
			return;		/* connection closed by other end */

			/* 4line from client specifies #bytes to write back */
		ntowrite = atol(line);
		if ((ntowrite <= 0) || (ntowrite > MAXN))
			err_quit("client request for %d bytes", ntowrite);

		Writen(sockfd, result, ntowrite);
	}
}

void *thread_main(void *arg) //子线程建立函数
{
	int		connfd;

	printf("thread %d starting\n", *(int *) arg);
	for ( ; ; ) {
    	Pthread_mutex_lock(&clifd_mutex); //使用互斥锁，确保只有一个工作线程接受任务分配
		while (iget == iput)
			Pthread_cond_wait(&clifd_cond, &clifd_mutex); //当主线程有任务下达时，通知某个睡眠在条件变量上的工作线程，注意已经接受任务的线程不会再次接受，除非已经处理完之前的任务
		connfd = clifd[iget];	/* 得到已连接套接字的编号 */
		if (++iget == MAXNCLI)
			iget = 0;
		Pthread_mutex_unlock(&clifd_mutex);
		tptr[*(int *) arg].thread_count++;
		web_child(connfd);		/* 处理工作线程 */
		Close(connfd);
		printf("工作线程 %d 目前已经处理了 %d 个任务\n", *(int *) arg,(int)tptr[*(int *) arg].thread_count);
	}
	free(arg); //线程退出前回收动态分配的空间
}

void thread_make(int i)
{
   iptr=Malloc(sizeof(int)); //对原来的程序做了修改，原来pthread_create中将值强转成i传递，现在改成动态分配
   *iptr=i;
	Pthread_create(&tptr[i].thread_tid, NULL, &thread_main,iptr);
	return;		/* 主线程返回 */
}

int main(int argc, char **argv)
{
	int			i, listenfd, connfd;
	socklen_t	addrlen, clilen;
	struct sockaddr	*cliaddr;

	if (argc == 3)
	{
		listenfd = Tcp_listen(NULL, argv[1], &addrlen);
		printf("端口号为%s\n",argv[1]);
	}
	else if (argc == 4)
		listenfd = Tcp_listen(argv[1], argv[2], &addrlen);
	else
		err_quit("usage: serv08 [ <host> ] <port#> <#threads>");
	cliaddr = Malloc(addrlen);

	nthreads = atoi(argv[argc-1]);
	tptr = Calloc(nthreads, sizeof(Thread)); //初始化Thread结构体数组
	iget = iput = 0;

		/* 预先生成线程池 */
	for (i = 0; i < nthreads; i++)
		thread_make(i);		/* 只有主线程返回 */

	printf("线程池生成完毕\n");

	int count;
	for (count=0; ;++count) {
		clilen = addrlen;
		connfd = Accept(listenfd, cliaddr, &clilen);
       printf("接受到新的连接%d\n",count);
		Pthread_mutex_lock(&clifd_mutex);
		clifd[iput] = connfd;
		if (++iput == MAXNCLI)
			iput = 0;
		if (iput == iget) //通常iput肯定比iget大，只有当clifd数组过小，导致iput清零后又追上iget
			err_quit("iput = iget = %d", iput);
		Pthread_cond_signal(&clifd_cond);  //发出信号，唤醒工作线程起床干活
		Pthread_mutex_unlock(&clifd_mutex);
	}
}

这是一个最基本的多线程服务器程序，阻塞式I/O加线程池，主线程接受连接后分给工作线程执行操作，工作线程中进行对I/O的阻塞式操作
 思考的问题：
1.服务器为每个已连接套接字都分配一个工作线程，要想满足大量的客户连接，必然需要增加线程池中工作线程的数量，这就存在一个问题，对于一个线程池来说，分配400个工作线程会比20个工作线程高效吗？显然不是，当工作线程的数量过大时，线程之间的切换会浪费大量的cpu时间，真正用来处理线程上任务的时间并不多。
一般需要根据任务的类型来配置线程池大小：
如果是CPU密集型任务，就需要尽量压榨CPU，参考值可以设为 NCPU+1
如果是IO密集型任务，参考值可以设置为2*NCPU
当然，这只是一个参考值，具体的设置还需要根据实际情况进行调整，比如可以先将线程池大小设置为参考值，再观察任务运行情况和系统负载、资源利用率来进行适当调整。
https://blog.csdn.net/weixin_42245374/article/details/82631951
https://blog.csdn.net/wxy941011/article/details/80879225
2.服务器为每个已连接套接字都分配一个工作线程，工作线程只处理这个套接字上的任务，套接字的读写操作由工作线程自己完成，但是工作线程使用阻塞I/O，如果此时客户一直不终止连接，则工作线程一直被挂起，无法执行其他任务，实际上效率很低。

改进的方法：
 将套接字的可读可写的监测放到主线程中，当主线程发现套接字可读或者可写时将读/写任务分配给工作线程去处理。

改进版本1：epoll水平触发方式+阻塞I/O
遇到的问题：
1.epoll在水平触发方式下可能会出现多个线程处理同一个套接字的情况，即epoll检测到套接字A可读，分配给工作线程thread1去处理，thread1很墨迹，当epoll_wait再次运行时还没将输入缓冲区中的数据处理完，此时服务器会认为有个新的读取任务来了，就分配另外一个线程thread2去处理，此时thread1和thread2同时都在处理套接字A，这显然是不安全的，可以使用EPOLLONESHOT来避免这种情况。
2.epoll是否是线程安全的，即是否可以同时在主线程和工作线程中对同一个epoll进行添加修改删除操作？操作时需要加互斥锁吗？
从查找到的资料来看，epoll_ctl是线程安全的，
当一个线程阻塞在epoll_wait（）上的时候，其他线程向其中添加新的文件描述符是没问题的，如果这个文件描述符就绪的话，阻塞线程的epoll_wait（）会被唤醒。但是如果正在监听的某文件描述符被其他线程关闭的话详情见select。
若一个文件描述符正被监听，其他线程关闭了的话，表现是未定义的。在有些 UNIX系统下，select会解除阻塞返回，而文件描述符会被认为就绪，然而对这个文件描述符进行IO操作会失败（除非这个文件描述符又被分配了），在Linux下，另一个线程关闭文件描述符没有任何影响。
总结：除了一个线程在epoll或者select中监控一个socket时候另外一个线程对这个socket进行close这种情况，我就可以认为多个线程操作同一个epoll fd的行为是安全的
https://blog.csdn.net/u011344601/article/details/51997886
3.每个工作线程都需要记录自己的线程编号，使用线程自带空间

extern "C" {
  #include <stdio.h>
  #include <stdlib.h>
  #include "unpthread.h"
}

#include <sys/epoll.h>
#include <iostream>
#include <queue>
using namespace std;

typedef struct {
  pthread_t		thread_tid;		/* 工作线程 ID */
  long			thread_count;	/* # 工作线程完成任务的计数器 */
} Thread;
Thread	*tptr;		/* 工作线程数组首地址 */

int		  *iptr,nwork; //nwork为当前的任务个数
queue<int> work_queue; //分配给工作线程的任务，分为读任务和写任务，每次分配分配任务先压入描述符号，再压入任务类别，0为读，1为写
queue<int> socket_close_queue; //需要关闭的套接字

static int			nthreads;
pthread_mutex_t		clifd_mutex = PTHREAD_MUTEX_INITIALIZER;
pthread_cond_t		clifd_cond = PTHREAD_COND_INITIALIZER;

int epfd,nfds; //epoll描述符，主线程和工作线程都只读，不用加锁

pthread_key_t r1_key;
pthread_once_t r1_once = PTHREAD_ONCE_INIT;

#define	MAXN	16384		/* max # bytes client can request */
void web_child(int sockfd,int work_mode)
{
	ssize_t		nread;
	char		line[MAXLINE], result[MAXN];

	if(!work_mode)
	{
		if ( (nread = Readline(sockfd, line, MAXLINE)) == 0)
					return;

		struct epoll_event ev_temp;
		ev_temp.data.fd=sockfd; //epoll是线程安全的，在主线程和工作线程对其进行操作
		ev_temp.events=EPOLLOUT | EPOLLONESHOT; //使用默认水平触发方式，注意水平触发下可能多个线程处理同一个套接字，需要设置EPOLLONESHOT
		epoll_ctl(epfd,EPOLL_CTL_MOD,sockfd,&ev_temp); //注册可写
	}
	else
	{
		Writen(sockfd, result, 4000);

		epoll_ctl(epfd,EPOLL_CTL_DEL,sockfd,NULL); //直接删除

	   //Close(sockfd);注意不可以直接在工作线程中关闭已连接的套接字，此时epoll_wait的反应是不确定的

		Pthread_mutex_lock(&clifd_mutex);
		socket_close_queue.push(sockfd);
		Pthread_mutex_unlock(&clifd_mutex);
	}
}

void threadid_destructor(void *ptr)
{
	free(ptr);
}

void threadid_once(void)
{
	pthread_key_create(&r1_key,threadid_destructor);
}

void *thread_main(void *arg) //子线程建立函数
{
	//先建立线程自带数据
	Pthread_once(&r1_once,threadid_once);
	int *ptr;
	if( (ptr = (int *)pthread_getspecific(r1_key)) == NULL)
	{
		ptr=(int *)Malloc(sizeof(int));
		Pthread_setspecific(r1_key,ptr);
	}
   *ptr=*(int *)arg; //在线程自带数据中记录线程id

   free(arg); //回收动态分配的内存

	int		connfd,work_mode;

	printf("thread %d starting\n", *(int *) ptr);
	for ( ; ; ) {

    	Pthread_mutex_lock(&clifd_mutex); //使用互斥锁，确保只有一个工作线程接受任务分配
		while (0==nwork)
			Pthread_cond_wait(&clifd_cond, &clifd_mutex); //当主线程有任务下达时，通知某个睡眠在条件变量上的工作线程，注意已经接受任务的线程不会再次接受，除非已经处理完之前的任务

		connfd = work_queue.front();	 /* 得到已连接套接字的编号 */
		work_queue.pop();
		work_mode=work_queue.front(); /* 得到操作类型 */
		work_queue.pop();
		printf("工作线程 %d 接受套接字 %d 任务类型 %d \n", *(int *) ptr,connfd,work_mode);
		--nwork; //完成一个任务
		Pthread_mutex_unlock(&clifd_mutex);

		tptr[*(int *) ptr].thread_count++;
		web_child(connfd,work_mode);		/* 处理工作线程 */

		printf("工作线程 %d 目前已经处理了 %d 个任务\n", *(int *) ptr,(int)tptr[*(int *) ptr].thread_count);
	}
	return NULL;
}

void thread_make(int i)
{
   iptr=(int *) Malloc(sizeof(int)); //对原来的程序做了修改，原来pthread_create中将值强转成i传递，现在改成动态分配,但是要记得回收
   *iptr=i;
	Pthread_create(&tptr[i].thread_tid, NULL, &thread_main,iptr);
	return;		/* 主线程返回 */
}

int main(int argc, char **argv)
{
	int	 i, listenfd, connfd;
	socklen_t	addrlen, clilen;
	struct sockaddr	*cliaddr;

	if (argc == 3)
	{
		listenfd = Tcp_listen(NULL, argv[1], &addrlen);
		printf("端口号为%s\n",argv[1]);
	}
	else if (argc == 4)
		listenfd = Tcp_listen(argv[1], argv[2], &addrlen);
	else
		err_quit("usage: serv08 [ <host> ] <port#> <#threads>");
	cliaddr = (sockaddr *)Malloc(addrlen);

	nthreads = atoi(argv[argc-1]);
	tptr = (Thread *)Calloc(nthreads, sizeof(Thread)); //初始化Thread结构体数组
		/* 预先生成线程池 */
	for (i = 0; i < nthreads; i++)
		thread_make(i);		/* 只有主线程返回 */
	printf("线程池生成完毕\n");

	struct epoll_event ev,events[100]; //ev用于注册事件，events数组用于返回要处理的事件
	//首先在epoll中注册监听套接字
	epfd=epoll_create(1);
	ev.data.fd=listenfd;
	ev.events=EPOLLIN; //使用默认水平触发方式
	epoll_ctl(epfd,EPOLL_CTL_ADD,listenfd,&ev);
	nwork=0;

	for( int count=0; ;count++)
	{
		//检查是否有套接字需要关闭
		Pthread_mutex_lock(&clifd_mutex);
       while(!socket_close_queue.empty())
        {
    	   Close(socket_close_queue.front());
    	   printf("成功关闭套接字%d\n",socket_close_queue.front());
    	   socket_close_queue.pop();
        }
      Pthread_mutex_unlock(&clifd_mutex);

		nfds=epoll_wait(epfd,events,100,-1);
      for(int i=0;i<nfds;++i)
       {
    	  if(events[i].data.fd==listenfd) //如果是监听套接字则说明有新的连接，此时接受，并注册新连接的可读
    	  {
    		  clilen = addrlen;
    		  connfd = Accept(listenfd, cliaddr, &clilen); //水平触发方式下，只要accept一次,注意边缘触发方式下多次accept
    		  //注册已连接套接字
    		  ev.data.fd=connfd;
    		  ev.events=EPOLLIN | EPOLLONESHOT; //使用默认水平触发方式，注意水平触发下可能多个线程处理同一个套接字，需要设置EPOLLONESHOT
    		  epoll_ctl(epfd,EPOLL_CTL_ADD,connfd,&ev);
    	  }
    	  else if(events[i].events & EPOLLIN )  //如果是已连接套接字，则判断是可读，将任务分给工作线程
    	  {
    		  //printf("监听到可读事件\n");
    		  Pthread_mutex_lock(&clifd_mutex);
    		  nwork++;
    		  work_queue.push(events[i].data.fd);
    		  work_queue.push(0);
    		  Pthread_cond_signal(&clifd_cond);  //发出信号，唤醒工作线程起床干活
    		  Pthread_mutex_unlock(&clifd_mutex);
    	  }
    	  else if(events[i].events & EPOLLOUT )  //如果是已连接套接字，则判断是可写，将任务分给工作线程
    	  {
    		  //printf("监听到可写事件\n");
    		  Pthread_mutex_lock(&clifd_mutex);
    		  nwork++;
    		  work_queue.push(events[i].data.fd);
    		  work_queue.push(1);
    		  Pthread_cond_signal(&clifd_cond);  //发出信号，唤醒工作线程起床干活
    		  Pthread_mutex_unlock(&clifd_mutex);
    	  }
       }
	}

}
