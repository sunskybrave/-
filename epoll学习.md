学习之余整理了网上的很多关于epoll的博客
**1.基本概念**

**用户空间 / 内核空间**
现在操作系统都是采用虚拟存储器，那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。
操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操作系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。

详细的请看

**进程切换**
为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的，并且进程切换是非常耗费资源的。

**进程阻塞**
正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得了CPU资源），才可能将其转为阻塞状态。当进程进入阻塞状态，是不占用CPU资源的。

**文件描述符**
文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。
文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。  
**缓存I/O**
缓存I/O又称为标准I/O，大多数文件系统的默认I/O操作都是缓存I/O。在Linux的缓存I/O机制中，操作系统会将I/O的数据缓存在文件系统的页缓存中，即数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。

**2.select、poll、epoll的比较**

select、poll 和 epoll 都是 Linux API 提供的 IO 复用方式

一般来说使用select和poll实现对套接字的监听，但是随着监听套接字数量的增长效率会降低，尤其是对于数千个监听描述符中只有几十个响应的情况，采用轮询方式实现监听大大影响了效率。

**select的缺点：**

1）单个进程能够监视的文件描述符的数量存在最大限制，通常是1024，当然可以更改数量，但由于select采用轮询的方式扫描文件描述符，文件描述符数量越多，性能越差；

2）内核 / 用户空间内存拷贝问题，select需要复制大量的句柄数据结构，产生巨大的开销；

select返回的是含有整个句柄的数组，应用程序需要遍历整个数组才能发现哪些句柄发生了事件。

具体来说：每次调用select，都需要把fd\_set集合从用户态拷贝到内核态，如果fd\_set集合很大时，那这个开销也很大；同时每次调用select都需要在内核遍历传递进来的所有fd\_set，如果fd\_set集合很大时，那这个开销也很大。

一个典型的场景如下：有100万个客户端同时与一个服务器进程保持着TCP连接。而每一时刻，通常只有几百上千个TCP连接是活跃的(事实上大部分场景都是这种情况)，此时select每次调用都会线性扫描全部的集合，导致效率呈现线性下降，极大的影响服务器的效率。

3）select的触发方式是水平触发，应用程序如果没有完成对一个已经就绪的文件描述符进行IO操作，那么之后每次select调用还是会将这些文件描述符通知进程，而epoll支持边缘触发，相比水平触发方式效率非常高，在并发，大流量的情况下，会比LT少很多epoll的系统调用，但是需要细致的处理每个请求，否则容易发生丢失事件的情况。

**Poll的缺点：**

相比select模型，poll使用链表保存文件描述符，因此没有了监视文件数量的限制，但是仍然存在上述的后两个缺点 **。**

**针对上述select和poll的缺点，epoll进行了改进**

**epoll的优点:**

epoll是Linux内核为处理大批量文件描述符而作了改进的poll，是Linux下多路复用IO接口select/poll的增强版本，它能显著提高程序在大量并发连接中只有少量活跃的情况下的系统CPU利用率。原因就是获取事件的时候，它无须遍历整个被侦听的描述符集，只要遍历那些被内核IO事件异步唤醒而加入Ready队列的描述符集合就行了。

**3.epoll的实现**

epoll的三大关键要素：mmap、红黑树、链表

epoll是通过内核与用户空间mmap同一块内存实现的。mmap将用户空间的一块地址和内核空间的一块地址同时映射到相同的一块物理内存地址（不管是用户空间还是内核空间都是虚拟地址，最终要通过地址映射映射到物理地址），使得这块物理内存对内核和对用户均可见，减少用户态和内核态之间的数据交换。内核可以直接看到epoll监听的句柄，效率高。

epoll在实现上采用红黑树去存储所有套接字（早期使用hash表存储），当添加或者删除一个套接字时（epoll\_ctl），都在红黑树上去处理，红黑树本身插入和删除性能比较好，时间复杂度O(logN)。

通过epoll\_ctl函数添加进来的事件都会被放在红黑树的某个节点内，所以，重复添加是没有用的。当把事件添加进来的时候时候会完成关键的一步，那就是该事件都会与相应的设备（网卡）驱动程序建立回调关系，当相应的事件发生后，就会调用这个回调函数，该回调函数在内核中被称为：ep\_poll\_callback,这个回调函数其实就所把这个事件添加到rdllist这个双向链表中。一旦有事件发生，epoll就会将该事件添加到双向链表中。那么当我们调用epoll\_wait时，epoll\_wait只需要检查rdlist双向链表中是否有存在注册的事件，效率非常可观。这里也需要将发生了的事件复制到用户态内存中即可。

epoll\_wait的工作流程：

1）epoll\_wait调用ep\_poll，当rdlist为空（无就绪fd）时挂起当前进程，直到rdlist不空时进程才被唤醒。

2）文件fd状态改变（buffer由不可读变为可读或由不可写变为可写），导致相应fd上的回调函数ep\_poll\_callback()被调用。

3）ep\_poll\_callback将相应fd对应epitem加入rdlist，导致rdlist不空，进程被唤醒，epoll\_wait得以继续执行。

4）ep\_events\_transfer函数将rdlist中的epitem拷贝到txlist中，并将rdlist清空。

5）ep\_send\_events函数（很关键），它扫描txlist中的每个epitem，调用其关联fd对用的poll方法。此时对poll的调用仅仅是取得fd上较新的events（防止之前events被更新），之后将取得的events和相应的fd发送到用户空间（封装在struct epoll\_event，从epoll\_wait返回）。

**4.epoll函数的使用**

Linux中提供的epoll相关函数如下：

int epoll\_create(int size);

1）epoll\_create函数创建一个epoll句柄，参数size表明内核要监听的描述符数量。调用成功时返回一个epoll句柄描述符，失败时返回-1。这里的size是早期设计时产生的，那时所有需监测的文件描述符放入hash表中，而现在时放入红黑树中，所以该参数现在并没有实际用到，但是必须

int epoll_ctl(int epfd, int op, int fd, struct epoll\_event \*event);

2）epoll_ctl函数注册要监听的事件类型。四个参数解释如下：

- epfd 表示epoll句柄
- op 表示fd操作类型，有如下3种
  - EPOLL_CTL_ADD 注册新的fd到epfd中
  - EPOLL_CTL_MOD 修改已注册的fd的监听事件
  - EPOLL_CTL_DEL 从epfd中删除一个fd
- fd 是要监听的描述符
- event 表示要监听的事件

epoll_event 结构体定义如下：

struct epoll_event {

    __uint32_t events;  \* Epoll events \*

    epoll_data_t data;  \* User data variable \*

};

typedef union epoll\_data {

    void \*ptr;

    int fd;

    \_\_uint32\_t u32;

    \_\_uint64\_t u64;

} epoll\_data\_t

events可以是以下几个宏的集合：

EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；

EPOLLOUT：表示对应的文件描述符可以写；

EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；

EPOLLERR：表示对应的文件描述符发生错误；

EPOLLHUP：表示对应的文件描述符被挂断；

EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。

EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里

int epoll\_wait(int epfd, struct epoll\_event \* events, int maxevents, int timeout);

3）epoll\_wait 函数等待事件的就绪，成功时返回就绪的事件数目，调用失败时返回 -1，等待超时返回 0。

epfd 是epoll句柄

events 表示从内核得到的就绪事件集合

maxevents 表示本次可以返回的最大事件数目，通常 maxevents参数与预分配的events数组的大小是相等的

timeout 表示在没有检测到事件发生时最多等待的时间（单位为毫秒），如果 timeout为0，则表示 epoll\_wait在 rdllist链表中为空，立刻返回，不会等待。

**5.epoll的触发方式**

epoll除了提供select/poll那种IO事件的水平触发（Level Triggered）外，还提供了边缘触发（Edge Triggered），这就使得用户空间程序有可能缓存IO状态，减少epoll\_wait/epoll\_pwait的调用，提高应用程序效率。

1)水平触发（LT）：默认工作模式，可以处理阻塞和非阻塞套接字，即当epoll\_wait检测到某描述符事件就绪并通知应用程序时，应用程序可以不立即处理该事件；下次调用epoll\_wait时，会再次通知此事件

LT模式详解

对于读操作

当buffer中有数据，且数据被读出一部分后buffer还不空的时候，即buffer中的内容减少的时候，LT模式返回读就绪。

对于写操作

当buffer不满，又写了一部分数据后扔然不满的的时候，即由于写操作的速度大于发送速度造成buffer中的内容增多的时候，LT模式会返回就绪。

LT模式优缺点：水平触发，效率会低于ET触发，尤其在大并发，大流量的情况下。但是LT对代码编写要求比较低，不容易出现问题。LT模式服务编写上的表现是：只要有数据没有被获取，内核就不断通知你，因此不用担心事件丢失的情况。

2)边缘触发（ET）：只支持非阻塞套接字，当epoll\_wait检测到某描述符事件就绪并通知应用程序时，应用程序必须立即处理该事件。如果不处理，下次调用epoll\_wait时，不会再次通知此事件。（直到你做了某些操作导致该描述符变成未就绪状态了，也就是说边缘触发只在状态由未就绪变为就绪时只通知一次）。

常见问题：ET模式为什么要设置在非阻塞模式下工作

因为ET模式下的读写需要一直读或写直到出错（对于读，当读到的实际字节数小于请求字节数时就可以停止），而如果你的文件描述符如果不是非阻塞的，那这个一直读或一直写势必会在最后一次阻塞。这样就不能在阻塞在epoll\_wait上了，造成其他文件描述符的任务饿死。

ET模式详解

对于读取操作：

1) 当buffer由不可读状态变为可读的时候，即由空变为不空的时候。

2) 当有新数据到达时，即buffer中的待读内容变多的时候。

另外补充一点：

3) 当buffer中有数据可读（即buffer不空）且用户对相应fd进行epoll\_mod IN事件时（具体见下节内容）。

对于写操作：

1) 当buffer由不可写变为可写的时候，即由满状态变为不满状态的时候。

2) 当有旧数据被发送走时，即buffer中待写的内容变少得时候。

另外补充一点：

3) 当buffer中有可写空间（即buffer不满）且用户对相应fd进行epoll\_mod OUT事件时

当epoll工作在ET模式下时，对于读操作，如果read一次没有读尽buffer中的数据，那么下次将得不到读就绪的通知，造成buffer中已有的数据无机会读出，除非有新的数据再次到达。对于写操作，主要是因为ET模式下fd通常为非阻塞造成的一个问题——如何保证将用户要求写的数据写完。

ET模式评价：边缘触发，效率非常高，在并发，大流量的情况下，会比LT少很多epoll的系统调用，因此效率高。但是对编程要求高，需要细致的处理每个请求，否则容易发生丢失事件的情况。

ET模式下的accept问题

请思考以下一种场景：在某一时刻，有多个连接同时到达，服务器的 TCP 就绪队列瞬间积累多个就绪连接，由于是边缘触发模式，epoll 只会通知一次，accept 只处理一个连接，导致 TCP 就绪队列中剩下的连接都得不到处理。在这种情形下，我们应该如何有效的处理呢？

解决的方法是：解决办法是用 while 循环抱住 accept 调用，处理完 TCP 就绪队列中的所有连接后再退出循环。如何知道是否处理完就绪队列中的所有连接呢？ accept  返回 -1 并且 errno 设置为 EAGAIN 就表示所有连接都处理完。

要解决上述ET模式下的读写问题，我们必须实现：

对于读，只要buffer中还有数据就一直读；

对于写，只要buffer还有空间且用户请求写的数据还未写完，就一直写。

总结：

LT：水平触发，效率会低于ET触发，尤其在大并发，大流量的情况下。但是LT对代码编写要求比较低，不容易出现问题。LT模式服务编写上的表现是：只要有数据没有被获取，内核就不断通知你，因此不用担心事件丢失的情况。

ET：边缘触发，效率非常高，在并发，大流量的情况下，会比LT少很多epoll的系统调用，因此效率高。但是对编程要求高，需要细致的处理每个请求，否则容易发生丢失事件的情况。

从本质上讲：与LT相比，ET模型是通过减少系统调用来达到提高并行效率的。

参考了以下的博客，致以感谢！  
Epoll模型详解  http://blog.chinaunix.net/uid-28541347-id-4288802.html  
Linux下的I/O复用与epoll详解  https://www.cnblogs.com/lojunren/p/3856290.html  
彻底学会使用epoll  http://blog.chinaunix.net/uid-28541347-id-4273856.html  
IO多路复用的三种机制Select，Poll，Epoll  https://www.jianshu.com/p/397449cadc9a  
